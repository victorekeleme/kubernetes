Blue Green Deployments
=======================
-> In a blue/green deployment strategy the old version of the application (green) and
    the new version (blue) get deployed at the same time.
-> When both of these are deployed, users only have access to the
    green; whereas, the blue is available to your QA team for test automation on a separate service or via direct port forwarding.
-> After the new version(blue) has been tested and is signed off for release, the service is switched to the blue version with the old green version being scaled down

Canary Deployments
===================
-> In Canary deployment, we deploy the new application and scale slowly to the cluster (appV1=90%, appV2=10%)
-> If no errors comes up as per requirement we can then keep sclaing till appV1 is completely taken down and appV2 takes over the cluster 


Resources
============
Requests -> Minimum guaranteed amount of a resource that is reserved for a container
Limits ->  the maximum amount of a resource to be used by a container. This means that the container can never consume more than the memory amount or CPU amount indicated.

How Scheduler checks for capacity per Node?
Capacity = Node - Sum all existing resources requests

If the process is running out of cpu --> Process(Application) Slow(Hung)
If the process running out of memory --> OOM killed (Out of memory)

Resource Configuration Values
Memory: is measured in bytes and expressed as an integer or using a fixed point integer
e.g:
  memory: 1 = 1 byte, 1Mi = 1 mebibyte/megabyte, 1Gi = 1 gigibyte/gigabyte
Note: If you put in a memory request that is larger than the amount of memory on your nodes, the pod will never be scheduled.

CPU: is measure in millicpus, which is 1/1000th of a CPU core and expressed with integers.
  e.g:
    cpu: 1 = 1 CPU, 2000m = 2 CPUs, 0.75 = 0.75 = 750m.

Note: One thing to keep in mind about CPU requests is that if you put in a value larger than the core count of your biggest node, your pod will never be scheduled

Scaling
============
HPA --> Horizontal Pod Autoscaler
  --> HPA will interact with the metrics server to identify CPU and Memory Utilization of POD by gathering information from kubelet running on the node
  --> HPA evaluates/observes the metrics on the metrics server in other scale in or out PODs in the cluster
  eg. minReplicas, maxReplicas
VPA --> Vertical Pod Autoscaler
